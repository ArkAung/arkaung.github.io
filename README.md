# Arkar Min Aung
## Deep Learning Engineer
* [Code](https://www.github.com/arkaung)
* [Writing](https://medium.com/@ark_aung)
* [Google Scholar](https://scholar.google.com/citations?user=KRZfXJQAAAAJ)
* [Social](https://twitter.com/ark_aung)
* Email: <aaung@pm.me>

### I organize
* [AI Discussions](https://arkaung.github.io/aidiscussions/)
* [FastAI Yangon Study Group](https://docs.google.com/document/d/1wLW3TnEtP2u4RAFHCIzkYw0HtyELjpbNV9U_lZpPDJs/edit?usp=sharing)
* Independent research group working on a novel active learning technique using knowledge distillation. For more info on this, please send me an email.

### Papers I recently read
* [Hydra: Preserving Ensemble Diversity for Model Distillation ](https://openreview.net/forum?id=ByeaXeBFvH)
* [Subclass distillation](https://arxiv.org/pdf/2002.03936.pdf)
* [Learning Active Learning from Data](https://papers.nips.cc/paper/7010-learning-active-learning-from-data.pdf)
* [AutoML-Zero: Evolving Machine Learning Algorithms From Scratch](https://arxiv.org/pdf/2003.03384.pdf)
